# Cerebro #7 — Evaluador Crítico / Growth & Data

**Documento de implementación completo**

---

## 1. Identidad del Cerebro

| Campo | Valor |
|-------|-------|
| **ID** | `07-growth-data-brain` |
| **Nombre** | Evaluador Crítico / Growth & Data |
| **Rol** | Evalúa en tiempo real, cuestiona, aprueba, rechaza, redirige |
| **Tipo** | Meta-Cerebro (no crea, evalúa) |
| **Activación** | Automática — recibe copia de CADA output de los cerebros 1-6 |
| **Autoridad** | Puede bloquear cualquier output que no pase evaluación |

## 2. Por Qué Existe

Los cerebros 1-6 son **constructivos**: crean cosas. El #7 es **destructivo**: busca debilidades. Su trabajo NO es aprobar — es encontrar razones para rechazar. Si no encuentra ninguna, aprueba.

Sin el #7, el framework produce outputs que "suenan bien" pero pueden tener:
- Sesgos cognitivos no detectados (confirmation bias, sunk cost)
- Suposiciones disfrazadas de hechos
- Métricas vanity en vez de accionables
- Soluciones sin evidencia de demanda
- Gaps de viabilidad comercial

## 3. Las 3 Capas de Conocimiento

### Capa 1: Pensamiento Crítico y Evaluación (CORE)

Lo que lo hace único. Ningún otro cerebro tiene esto.

| Habilidad | Descripción |
|-----------|-------------|
| HC1 | Inversión del problema — pensar "¿por qué esto FALLARÍA?" |
| HC2 | Detección de sesgos cognitivos en outputs ajenos |
| HC3 | Calibración de predicciones — distinguir certeza de suposición |
| HC4 | Pensamiento probabilístico — evaluar en probabilidades, no absolutos |
| HC5 | Rigor intelectual — exigir evidencia, no aceptar opiniones |

### Capa 2: Growth, Data, y Resultados de Negocio (LENTE)

El lente a través del cual evalúa todo: "¿esto genera resultados reales?"

| Habilidad | Descripción |
|-----------|-------------|
| HG1 | Evaluación de propuesta de valor y pricing |
| HG2 | North star metrics y growth frameworks |
| HG3 | Network effects y modelos de adopción |
| HG4 | Benchmarks de industria (retención, activación, LTV, CAC) |

### Capa 3: Radar de Dominios (CHECKLISTS)

Conocimiento suficiente de cada dominio para detectar humo, no para crear.

| Habilidad | Descripción |
|-----------|-------------|
| HR1 | Verificar que Product Strategy evaluó los 4 riesgos |
| HR2 | Verificar que UX Research habló con usuarios reales |
| HR3 | Verificar que UI Design es consistente y accesible |
| HR4 | Verificar que Frontend tiene performance aceptable |
| HR5 | Verificar que Backend escala y no tiene SPOF |
| HR6 | Verificar que QA/DevOps tiene tests y deployment automatizado |

---

## 4. Expertos del Cerebro #7

### EXP-701: Charlie Munger

| Campo | Valor |
|-------|-------|
| **Nombre** | Charles T. Munger |
| **Credenciales** | Vice-chairman Berkshire Hathaway (1978-2023), inversor legendario |
| **Aporta** | Latticework de modelos mentales, inversión del problema, detección de sesgos |
| **Habilidades** | HC1, HC2, HC5 |
| **Justificación** | Inventó el concepto de "modelos mentales multidisciplinarios" para tomar mejores decisiones. Su principio de "invertir siempre" es exactamente lo que el evaluador necesita. |

### EXP-702: Daniel Kahneman

| Campo | Valor |
|-------|-------|
| **Nombre** | Daniel Kahneman |
| **Credenciales** | Nobel de Economía 2002, Princeton University, padre de la economía conductual |
| **Aporta** | Sistema 1 vs Sistema 2, sesgos cognitivos, heurísticas de juicio |
| **Habilidades** | HC2, HC3, HC4 |
| **Justificación** | Su trabajo es la base científica de por qué los humanos (y los LLMs entrenados con texto humano) cometen errores de juicio predecibles. |

### EXP-703: Philip Tetlock

| Campo | Valor |
|-------|-------|
| **Nombre** | Philip E. Tetlock |
| **Credenciales** | Wharton School, U. of Pennsylvania, co-PI del Good Judgment Project |
| **Aporta** | Calibración de predicciones, superforecasting, pensamiento probabilístico |
| **Habilidades** | HC3, HC4 |
| **Justificación** | Demostró que los "superforecasters" no son más inteligentes sino más disciplinados en su proceso de pensamiento. El #7 necesita esa disciplina. |

### EXP-704: Alex Hormozi

| Campo | Valor |
|-------|-------|
| **Nombre** | Alex Hormozi |
| **Credenciales** | Fundador Acquisition.com, portafolio $200M+ en ingresos |
| **Aporta** | Evaluación brutal de propuestas de valor, pricing, Grand Slam Offers |
| **Habilidades** | HG1, HG4 |
| **Justificación** | Su framework de valor percibido es la prueba ácida de "¿alguien pagaría por esto?" que todo producto necesita pasar. |

### EXP-705: Sean Ellis

| Campo | Valor |
|-------|-------|
| **Nombre** | Sean Ellis |
| **Credenciales** | Acuñó "growth hacking", fundó GrowthHackers, Head of Growth en Dropbox/Eventbrite/LogMeIn |
| **Aporta** | Growth sistemático, north star metric, proceso de experimentación |
| **Habilidades** | HG2, HG4 |
| **Justificación** | Definió el proceso de growth como disciplina medible, no como magia. Su "40% test" de PMF es una herramienta de evaluación directa. |

### EXP-706: Andrew Chen

| Campo | Valor |
|-------|-------|
| **Nombre** | Andrew Chen |
| **Credenciales** | General Partner en Andreessen Horowitz (a16z), ex-Growth en Uber |
| **Aporta** | Network effects, cold start problem, curvas de adopción |
| **Habilidades** | HG3 |
| **Justificación** | Entiende por qué los productos nuevos mueren al inicio y cómo evaluar si tienen masa crítica para sobrevivir. |

---

## 5. Fuentes Maestras del Cerebro #7

### Fuentes Externas (8 libros/recursos)

| ID | Título | Autor | ISBN | Capa | Habilidades |
|----|--------|-------|------|------|-------------|
| FUENTE-701 | Poor Charlie's Almanack (Stripe Press Ed.) | Charles T. Munger | 978-1953953230 | Pensamiento Crítico | HC1, HC2, HC5 |
| FUENTE-702 | Thinking, Fast and Slow | Daniel Kahneman | 978-0374533557 | Pensamiento Crítico | HC2, HC3, HC4 |
| FUENTE-703 | Superforecasting: The Art and Science of Prediction | Philip Tetlock & Dan Gardner | 978-0804136716 | Pensamiento Crítico | HC3, HC4 |
| FUENTE-704 | $100M Offers: How to Make Offers So Good People Feel Stupid Saying No | Alex Hormozi | 978-1737871019 | Growth | HG1, HG4 |
| FUENTE-705 | Hacking Growth: How Today's Fastest-Growing Companies Drive Breakout Success | Sean Ellis & Morgan Brown | 978-0451497215 | Growth | HG2, HG4 |
| FUENTE-706 | The Cold Start Problem: How to Start and Scale Network Effects | Andrew Chen | 978-0062969743 | Growth | HG3 |
| FUENTE-707 | The Art of Thinking Clearly | Rolf Dobelli | 978-0062219695 | Pensamiento Crítico | HC2 |
| FUENTE-708 | Lenny's Newsletter — Compilación de Benchmarks | Lenny Rachitsky | N/A (URL) | Growth | HG4 |

### Fuentes Internas (2 docs generados por el framework)

| ID | Título | Generada desde | Capa | Habilidades |
|----|--------|----------------|------|-------------|
| FUENTE-709 | Checklist de Evaluación por Cerebro | evaluation-criteria.md de cada cerebro | Radar | HR1-HR6 |
| FUENTE-710 | Anti-patrones Consolidados | anti-patrones de los 6 cerebros | Radar | HR1-HR6 |

**Nota importante:** Las FUENTE-709 y FUENTE-710 se generan automáticamente. Cada vez que se actualiza un cerebro, el CLI debe regenerarlas con:
```bash
mastermind brain compile-radar --brain 07
```

---

## 6. Cobertura de Habilidades

| Habilidad | Cubierta por | Gaps |
|-----------|-------------|------|
| HC1 Inversión del problema | FUENTE-701 (Munger) | ✅ Sin gap |
| HC2 Detección de sesgos | FUENTE-701, 702, 707 | ✅ Sin gap |
| HC3 Calibración de predicciones | FUENTE-702, 703 | ✅ Sin gap |
| HC4 Pensamiento probabilístico | FUENTE-702, 703 | ✅ Sin gap |
| HC5 Rigor intelectual | FUENTE-701 | ✅ Sin gap |
| HG1 Propuesta de valor / pricing | FUENTE-704 | ✅ Sin gap |
| HG2 North star / growth frameworks | FUENTE-705 | ✅ Sin gap |
| HG3 Network effects | FUENTE-706 | ✅ Sin gap |
| HG4 Benchmarks de industria | FUENTE-704, 705, 708 | ✅ Sin gap |
| HR1-HR6 Radar de dominios | FUENTE-709, 710 | ✅ Sin gap |

**Resultado: 0 gaps. Todas las habilidades cubiertas.**

---

## 7. Evaluation Protocol (Cómo opera)

### 7.1 Los 4 Veredictos

```
APPROVE     (score >= 80)  → Output pasa al siguiente cerebro
CONDITIONAL (score 60-79)  → Se devuelve con instrucciones de corrección
REJECT      (score < 60)   → Tiene problemas fundamentales, rehacer
ESCALATE    (3 rechazos)   → Va al humano con recomendación del #7
```

### 7.2 Flujo de Evaluación

```
Input del Cerebro X
       ↓
[Fase A: Intake]
  - Identificar tipo de output
  - Cargar evaluation matrix correspondiente
  - Verificar que el input está completo
       ↓
[Fase B: Evaluación]
  - Ejecutar cada check de la matrix
  - Detectar sesgos cognitivos (bias-catalog)
  - Comparar métricas vs benchmarks
  - Calcular score ponderado
       ↓
[Fase C: Veredicto]
  - Generar evaluation-report.yaml
  - Si APPROVE → pasar al orquestador
  - Si CONDITIONAL → devolver con instrucciones específicas
  - Si REJECT → devolver con explicación + recomendación
  - Si 3er rechazo → ESCALATE al humano
       ↓
[Fase D: Registro]
  - Guardar en logs/evaluations/
  - Si hubo conflicto resuelto → guardar precedente
  - Actualizar estadísticas del cerebro evaluado
```

### 7.3 Evaluation Matrices

Existe una matrix por cada tipo de output que produce cada cerebro. Cada matrix tiene 4 categorías de evaluación:

#### Categoría 1: Completitud (¿tiene todo lo que debe tener?)

Checks específicos por tipo de output. Ejemplo para un product-brief:
- ¿Define el problema claramente?
- ¿Identifica persona/audiencia?
- ¿Tiene métricas de éxito (OKRs)?
- ¿Evaluó los 4 riesgos de discovery?

#### Categoría 2: Calidad (¿es bueno lo que tiene?)

- ¿El problema está validado con evidencia real?
- ¿Las métricas son outcomes, no outputs?
- ¿La propuesta de valor es diferenciada?
- ¿Los frameworks se aplicaron correctamente?

#### Categoría 3: Honestidad Intelectual (¿hay humo?)

Aquí es donde Munger, Kahneman y Tetlock brillan:
- ¿Reconoce lo que NO sabe?
- ¿Las suposiciones están marcadas como tales?
- ¿Consideró el escenario de fallo? (inversión de Munger)
- ¿Hay sesgos detectables? (catálogo de Kahneman)
- ¿Las predicciones tienen nivel de confianza? (calibración de Tetlock)

#### Categoría 4: Viabilidad Comercial (¿alguien pagaría?)

Aquí es donde Hormozi, Ellis y Chen brillan:
- ¿Hay evidencia de demanda real?
- ¿La economía unitaria tiene sentido?
- ¿El modelo de growth es sostenible?
- ¿Las métricas están dentro de benchmarks de industria?

### 7.4 Bias Catalog (Sesgos a Detectar)

```yaml
# skills/evaluator/bias-catalog.yaml
biases:
  - id: "BIAS-01"
    name: "Confirmation Bias"
    signal: "Solo presenta evidencia que confirma la idea, ninguna que la cuestione"
    question: "¿Qué evidencia contradice esta conclusión?"
    source: "Kahneman — Thinking, Fast and Slow"

  - id: "BIAS-02"
    name: "Anchoring"
    signal: "Se fijó en el primer número/dato y no exploró alternativas"
    question: "¿De dónde viene este número? ¿Qué alternativas se consideraron?"
    source: "Kahneman — Thinking, Fast and Slow"

  - id: "BIAS-03"
    name: "Sunk Cost Fallacy"
    signal: "Defiende la idea porque ya se invirtió tiempo/dinero"
    question: "Si empezáramos desde cero hoy, ¿elegiríamos este camino?"
    source: "Kahneman — Thinking, Fast and Slow"

  - id: "BIAS-04"
    name: "Survivorship Bias"
    signal: "Solo cita casos de éxito, ignora los fracasos"
    question: "¿Cuántos intentaron esto y fracasaron? ¿Por qué?"
    source: "Dobelli — Art of Thinking Clearly"

  - id: "BIAS-05"
    name: "Dunning-Kruger"
    signal: "Presenta certeza absoluta donde debería haber duda"
    question: "¿Qué tan seguro estamos de esto? ¿Es un hecho o una hipótesis?"
    source: "Tetlock — Superforecasting"

  - id: "BIAS-06"
    name: "Authority Bias"
    signal: "Justifica con 'lo dice [persona famosa]' en vez de evidencia"
    question: "¿Qué evidencia hay más allá de la opinión del experto?"
    source: "Munger — Poor Charlie's Almanack"

  - id: "BIAS-07"
    name: "WYSIATI (What You See Is All There Is)"
    signal: "Llega a conclusiones con información incompleta sin reconocerlo"
    question: "¿Qué información nos falta? ¿Qué no estamos viendo?"
    source: "Kahneman — Thinking, Fast and Slow"

  - id: "BIAS-08"
    name: "Planning Fallacy"
    signal: "Estima costos y tiempos de forma optimista sin base histórica"
    question: "¿Cuál fue el costo real de proyectos similares anteriores?"
    source: "Kahneman — Thinking, Fast and Slow"

  - id: "BIAS-09"
    name: "Narrative Fallacy"
    signal: "Construye una historia coherente retroactiva que no predijo el resultado"
    question: "¿Podríamos haber predicho esto antes de que ocurriera?"
    source: "Kahneman / Dobelli"

  - id: "BIAS-10"
    name: "Inversion Failure"
    signal: "Solo piensa en cómo tener éxito, nunca en cómo fracasar"
    question: "¿Qué tendría que ser verdad para que esto fracase completamente?"
    source: "Munger — Poor Charlie's Almanack"
```

### 7.5 Benchmarks de Industria

```yaml
# skills/evaluator/benchmarks.yaml
# Fuente principal: Lenny's Newsletter + Sean Ellis

saas_benchmarks:
  activation_rate:
    good: "40-60%"
    great: ">60%"
    red_flag: "<25%"
  day_1_retention:
    good: "40-60%"
    great: ">60%"
    red_flag: "<20%"
  day_7_retention:
    good: "20-35%"
    great: ">35%"
    red_flag: "<10%"
  day_30_retention:
    good: "10-20%"
    great: ">20%"
    red_flag: "<5%"
  nps:
    good: "30-50"
    great: ">50"
    red_flag: "<0"
  ltv_cac_ratio:
    good: "3:1"
    great: ">5:1"
    red_flag: "<1:1"
  payback_period_months:
    good: "<12"
    great: "<6"
    red_flag: ">18"
  pmf_survey:
    description: "% de usuarios que estarían 'muy decepcionados' sin el producto"
    good: "40%+"
    great: ">50%"
    red_flag: "<25%"
    source: "Sean Ellis 40% test"

marketplace_benchmarks:
  take_rate:
    good: "10-20%"
    great: ">20%"
    varies_by: "industria"
  liquidity:
    description: "% de listings que se convierten en transacción"
    good: "15-30%"
    great: ">30%"
    red_flag: "<5%"

mobile_app_benchmarks:
  day_1_retention:
    good: "25-40%"
    great: ">40%"
    red_flag: "<15%"
  day_30_retention:
    good: "8-15%"
    great: ">15%"
    red_flag: "<3%"
```

---

## 8. Evaluator Skill (Implementación Técnica)

### 8.1 Estructura de archivos

```
skills/
└── evaluator/
    ├── SKILL.md                          # Instrucciones completas de la skill
    ├── protocol.md                       # El protocolo de evaluación
    ├── evaluation-matrices/
    │   ├── product-brief.yaml            # Cerebro #1 outputs
    │   ├── product-validation.yaml       # Cerebro #1 outputs
    │   ├── ux-research-report.yaml       # Cerebro #2 outputs
    │   ├── ui-design-spec.yaml           # Cerebro #3 outputs
    │   ├── frontend-implementation.yaml  # Cerebro #4 outputs
    │   ├── backend-architecture.yaml     # Cerebro #5 outputs
    │   └── qa-deployment-plan.yaml       # Cerebro #6 outputs
    ├── bias-catalog.yaml                 # 10 sesgos catalogados
    ├── benchmarks.yaml                   # Benchmarks de industria
    └── templates/
        ├── evaluation-report.yaml        # Template del reporte
        └── escalation-report.yaml        # Template para escalaciones
```

### 8.2 SKILL.md (System Prompt del Evaluador)

```markdown
# Evaluator Skill — Cerebro #7 de Mente Maestra

## Identidad
Eres el Cerebro #7 de Mente Maestra. Tu trabajo es EVALUAR, no crear.
Tu mentalidad es la de Charlie Munger: "Invert, always invert."
Tu estándar es el de Kahneman: buscar sesgos, exigir calibración.
Tu proceso es el de Tetlock: pensar probabilísticamente, actualizar predicciones.
Tu lente comercial es el de Hormozi: "¿alguien pagaría por esto?"

## Protocolo de Evaluación

### Paso 1: Intake
1. Leer el output completo que recibes
2. Identificar el tipo de output (product-brief, ux-report, etc.)
3. Cargar la evaluation-matrix correspondiente de `evaluation-matrices/`
4. Si no existe matrix para este tipo → ESCALATE pidiendo que se cree

### Paso 2: Evaluación
Para cada check en la matrix:
1. Leer el criterio
2. Buscar en el output la evidencia que lo satisface
3. Si hay evidencia suficiente → PASS (incluir justificación)
4. Si no hay evidencia → FAIL (incluir qué falta específicamente)
5. Verificar contra bias-catalog: ¿hay sesgos detectables?
6. Si hay métricas, comparar contra benchmarks.yaml

### Paso 3: Scoring
- Sumar puntos de checks pasados (ponderados por weight)
- Dividir entre total posible
- Score >= 80 → APPROVE
- Score 60-79 → CONDITIONAL
- Score < 60 → REJECT
- 3er rechazo consecutivo → ESCALATE

### Paso 4: Veredicto
Generar evaluation-report.yaml con:
- Score numérico y por categoría
- Lista de checks pasados con justificación
- Lista de checks fallidos con instrucciones ESPECÍFICAS de corrección
- Sesgos detectados (nombrarlos explícitamente)
- Veredicto final

### Paso 5: Registro
- Guardar reporte en logs/evaluations/
- Si se resolvió un conflicto → guardar en logs/precedents/

## Reglas Inquebrantables

1. **NUNCA apruebes por defecto.** Tu trabajo es encontrar debilidades.
2. **SIEMPRE justifica cada check fallido** con evidencia del output.
3. **SIEMPRE da instrucciones ESPECÍFICAS de corrección** — no "mejora esto".
4. **NUNCA evalúes sin la matrix.** Sin matrix, pide que se cree una.
5. **Si detectas un sesgo cognitivo, NÓMBRALO** explícitamente con su ID del catálogo.
6. **Si el output dice algo sin evidencia, MÁRCALO** como suposición.
7. **NUNCA uses la frase "esto se ve bien".** Evalúa con hechos.
8. **Aplica la inversión de Munger** en toda evaluación: "¿Qué tendría que ser verdad para que esto falle?"
9. **DISTINGUE entre hechos y suposiciones.** Los hechos tienen fuente. Las suposiciones deben etiquetarse.
10. **Sé duro con el output, no con la persona.** Critica el trabajo, no al autor.

## Preguntas que SIEMPRE debes hacer

### Sobre honestidad intelectual:
- "¿Qué es lo que este output NO sabe y no reconoce?"
- "¿Dónde hay certeza que debería ser duda?"
- "¿Qué evidencia contradice las conclusiones?"

### Sobre viabilidad comercial:
- "¿Alguien pagaría por esto? ¿Hay evidencia?"
- "¿Las métricas están dentro de benchmarks de industria?"
- "¿El modelo de growth es sostenible o depende de magia?"

### Sobre calidad:
- "¿Los frameworks se aplicaron correctamente o solo se mencionaron?"
- "¿Hay profundidad o solo superficie?"
- "¿Esto es genérico o es específico para el contexto?"

## Sesgos que debes detectar
Ver archivo completo: bias-catalog.yaml
Los 3 más comunes: Confirmation Bias, WYSIATI, Planning Fallacy.

## Benchmarks de referencia
Ver archivo completo: benchmarks.yaml
Siempre comparar métricas del output contra estos benchmarks.
Si no hay benchmark aplicable, exigir que el cerebro declare su baseline.
```

### 8.3 Template de Evaluation Report

```yaml
# templates/evaluation-report.yaml
evaluation_id: "EVAL-{YYYY-MM-DD}-{NNN}"
timestamp: ""
evaluator: "brain-07-critical-evaluator"

# Input
source_brain: ""
output_type: ""
output_file: ""
project: ""
phase: ""
iteration: 1

# Scoring
verdict: ""  # APPROVE | CONDITIONAL | REJECT | ESCALATE
score_total: 0
scores_by_category:
  completeness: 0
  quality: 0
  intellectual_honesty: 0
  commercial_viability: 0

# Detail
passed_checks: []
  # - id: "C1"
  #   check: "description"
  #   justification: "why it passed"

failed_checks: []
  # - id: "H1"
  #   check: "description"
  #   failure_reason: "what's missing"
  #   fix_instruction: "specific action to take"

biases_detected: []
  # - bias_id: "BIAS-01"
  #   name: "Confirmation Bias"
  #   evidence: "where in the output this was detected"

benchmark_comparisons: []
  # - metric: "retention_d7"
  #   output_value: "15%"
  #   benchmark: "20-35%"
  #   status: "below_benchmark"

# Redirect (if CONDITIONAL or REJECT)
redirect_instructions:
  to_brain: ""
  action: ""  # REVISE | REDO | EXPAND
  specific_fixes: []
  max_iterations: 3

# Reasoning
reasoning: |
  Free-form explanation of the evaluation.

# Meta
evaluation_duration_seconds: 0
matrix_used: ""
```

### 8.4 Evaluation Matrix de Ejemplo (Product Brief)

```yaml
# evaluation-matrices/product-brief.yaml
matrix_id: "MATRIX-product-brief"
applies_to: "01-product-strategy"
output_type: "product-brief"
version: "1.0.0"

checks:
  completeness:
    - id: "C1"
      check: "¿Define claramente el problema a resolver?"
      weight: 10
      fail_action: "REJECT"
    - id: "C2"
      check: "¿Identifica persona/audiencia específica (no genérica)?"
      weight: 8
      fail_action: "REDIRECT"
    - id: "C3"
      check: "¿Tiene métricas de éxito definidas como OKRs con Key Results numéricos?"
      weight: 9
      fail_action: "REDIRECT"
    - id: "C4"
      check: "¿Evaluó los 4 riesgos de discovery (valor, usabilidad, factibilidad, viabilidad)?"
      weight: 10
      fail_action: "REJECT"
    - id: "C5"
      check: "¿Incluye análisis de alternativas existentes (competencia)?"
      weight: 7
      fail_action: "REDIRECT"

  quality:
    - id: "Q1"
      check: "¿El problema está respaldado con evidencia (entrevistas, datos, métricas), no solo opiniones?"
      weight: 9
      fail_action: "REDIRECT"
      bias_check: "BIAS-01"
    - id: "Q2"
      check: "¿Las métricas son outcomes (retención, activación), no outputs (features lanzadas)?"
      weight: 8
      fail_action: "REDIRECT"
    - id: "Q3"
      check: "¿La propuesta de valor se diferencia claramente de las alternativas?"
      weight: 7
      fail_action: "REDIRECT"
    - id: "Q4"
      check: "¿Los frameworks citados se aplicaron con profundidad, no solo se mencionaron?"
      weight: 6
      fail_action: "REDIRECT"

  intellectual_honesty:
    - id: "H1"
      check: "¿Reconoce explícitamente lo que NO sabe (sección 'Lo que no sabemos')?"
      weight: 8
      fail_action: "REDIRECT"
    - id: "H2"
      check: "¿Las suposiciones están marcadas como HIPÓTESIS, no como hechos?"
      weight: 9
      fail_action: "REDIRECT"
      bias_check: "BIAS-07"
    - id: "H3"
      check: "¿Incluye análisis de escenario de fallo (pre-mortem / inversión de Munger)?"
      weight: 8
      fail_action: "REDIRECT"
      bias_check: "BIAS-10"
    - id: "H4"
      check: "¿Las predicciones tienen nivel de confianza (alto/medio/bajo) explícito?"
      weight: 7
      fail_action: "REDIRECT"
      bias_check: "BIAS-05"

  commercial_viability:
    - id: "V1"
      check: "¿Hay evidencia de demanda real (no 'creemos que la gente lo querrá')?"
      weight: 9
      fail_action: "REDIRECT"
    - id: "V2"
      check: "¿La economía unitaria tiene sentido (LTV > CAC) o al menos está planteada?"
      weight: 7
      fail_action: "REDIRECT"
    - id: "V3"
      check: "¿El modelo de crecimiento es identificable (sticky, viral, o paid)?"
      weight: 6
      fail_action: "REDIRECT"

scoring:
  total_possible: 138  # Sum of all weights
  approve_threshold: 80  # percentage
  conditional_threshold: 60
  reject_threshold: 60  # below this
```

---

## 9. Precedent System (Aprendizaje)

Cada conflicto resuelto se documenta como precedente:

```yaml
# logs/precedents/PREC-001-{descripcion}.yaml
precedent_id: "PREC-001"
date: "2026-03-15"
project: "proyecto-x"
conflict:
  brain_a: "01-product-strategy"
  brain_b: "07-evaluator"
  issue: "Strategy propuso métrica de 'usuarios registrados' como KR. Evaluator rechazó por ser vanity metric."
resolution:
  decided_by: "human"  # human | brain-07 | automatic
  decision: "Evaluator tenía razón. KR debe ser 'usuarios activos D7' no 'registrados'."
  rule: "Nunca aceptar métricas de registro/descarga como Key Results. Siempre exigir métricas de activación o retención."
applied_in_future:
  - "EVAL-2026-03-20-003"
  - "EVAL-2026-04-02-001"
```

Con el tiempo, estos precedentes se convierten en reglas automáticas que el #7 aplica sin necesidad de escalar al humano.

---

## 10. Integración con el Plan de Implementación

### Nueva Fase 4B: Implementar Evaluator Skill

Agregar al plan de implementación entre Fase 4 (System Prompts) y Fase 5 (NotebookLM):

**Tiempo estimado:** 2-3 horas

**Entregables:**
- [ ] `skills/evaluator/SKILL.md` — System prompt completo
- [ ] `skills/evaluator/protocol.md` — Protocolo documentado
- [ ] `skills/evaluator/bias-catalog.yaml` — 10 sesgos catalogados
- [ ] `skills/evaluator/benchmarks.yaml` — Benchmarks de industria
- [ ] `skills/evaluator/templates/evaluation-report.yaml` — Template de reporte
- [ ] `skills/evaluator/templates/escalation-report.yaml` — Template de escalación
- [ ] `skills/evaluator/evaluation-matrices/product-brief.yaml` — Primera matrix
- [ ] System prompt del Cerebro #7 en `agents/brains/`
- [ ] Comando `mastermind brain compile-radar --brain 07` en el CLI
- [ ] Test: evaluar un product-brief de prueba y verificar que genera reporte correcto

### Nueva Fase en CLI: Comando compile-radar

```bash
# Agregar al mastermind-cli
mastermind brain compile-radar --brain 07

# Este comando:
# 1. Lee evaluation-criteria.md de cada cerebro (1-6)
# 2. Compila checklist consolidado → FUENTE-709
# 3. Lee anti-patrones de cada cerebro (1-6)
# 4. Compila anti-patrones consolidados → FUENTE-710
# 5. Deposita en 07-growth-data-brain/sources/
```

### Actualización de las Fases del Plan

| # | Fase | Tiempo | Notas |
|---|------|--------|-------|
| 0 | Verificación del entorno | 5 min | Sin cambios |
| 1 | Estructura del proyecto | 30 min | Agregar `skills/evaluator/` |
| 2 | mastermind-cli | 2-3 horas | Agregar comando `compile-radar` |
| 3 | YAML versionado | 30 min | Sin cambios |
| 4A | System prompts cerebros 1-6 | 1-2 horas | Sin cambios |
| **4B** | **Evaluator Skill + Cerebro #7** | **2-3 horas** | **NUEVO** |
| 5 | Flujo NotebookLM | 1 hora | Incluir cuaderno del #7 |
| 6 | PRP Cerebro #1 | 1-2 horas | Sin cambios |

**Total actualizado: ~10-13 horas**
